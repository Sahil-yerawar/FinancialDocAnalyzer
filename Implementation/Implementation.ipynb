{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of financial_documents.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "udPUMQA_l-7a",
        "NCOkJfDTEHti",
        "41POamTPD5Od",
        "hPd6xa5uD9Wp"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mfoQm3EBJK4s",
        "outputId": "3f03ab71-8fa9-44bf-f384-dc7b773003e6"
      },
      "source": [
        "!apt install pdfgrep libmagickwand-dev\n",
        "!pip install pdftotree camelot-py pdf2image nltk"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/site-packages (3.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/site-packages (from nltk) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4lpzAzKnI87"
      },
      "source": [
        "!wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KqsNHy8anRrz"
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, \"/usr/local/lib/python3.6/site-packages/\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NYZwxXBFnSZW"
      },
      "source": [
        "!conda install -c conda-forge python=3.6 tesseract poppler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UncwUEW9Sqg3",
        "outputId": "bb9d74e7-720c-4468-b2dc-cf21894b4a7a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXsR9IEUNIlZ"
      },
      "source": [
        "### Extracting Financial Data from the list of PDF's"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmXslG7uNOod"
      },
      "source": [
        "# import all the required libraries\n",
        "import os\n",
        "import pdftotree\n",
        "import subprocess\n",
        "from subprocess import check_output, Popen\n",
        "from pdf2image import convert_from_path\n",
        "import re\n",
        "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
        "# import fitz\n",
        "from nltk.tag import StanfordNERTagger\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "st = StanfordNERTagger('/content/drive/My Drive/startup/StanfordNLP/stanford-ner-2020-11-17/classifiers/english.all.3class.distsim.crf.ser.gz',\n",
        "\t\t\t\t\t   '/content/drive/My Drive/startup/StanfordNLP/stanford-ner-2020-11-17/stanford-ner.jar',\n",
        "\t\t\t\t\t   encoding='utf-8')\n",
        "nltk.download('punkt')\n",
        "def get_num_pages(pdf_path):\n",
        "    output = check_output([\"pdfinfo\", pdf_path]).decode()\n",
        "    pages_line = [line for line in output.splitlines() if \"Pages:\" in line][0]\n",
        "    num_pages = int(pages_line.split(\":\")[1])\n",
        "    return num_pages"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MyERCQvlYK9d"
      },
      "source": [
        "# Cell for File Paths\n",
        "pdf_dir = \"/content/drive/My Drive/startup/interview_data/\"\n",
        "temp_dir = \"/content/drive/My Drive/startup/test_jpg/\"\n",
        "text_dir = \"/content/drive/My Drive/startup/all_text_cr/\"\n",
        "img_path = \"/content/drive/My Drive/startup/imgpath.txt\"\n",
        "csv_file = \"/content/drive/My Drive/startup/final.csv\"\n",
        "bse_companies = \"/content/drive/My Drive/startup/bse_companies.csv\"\n",
        "large_pdfs = \"/content/drive/My Drive/startup/large_pdfs/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zs2jQeBlMkzH"
      },
      "source": [
        "# Since we are referring to the bse companies csv file, we need to store the map from different codes to company names\n",
        "f = open(bse_companies,\"rb\")\n",
        "bb_map = {}\n",
        "nse_map = {}\n",
        "bse_map = {}\n",
        "# print(f.readlines())\n",
        "for l in f.readlines():\n",
        "  print(type(l),l)\n",
        "  ld = l.decode('utf-8').split(',')\n",
        "  if (ld[4] != ''):\n",
        "    nse_map[ld[4]] = ld[2]\n",
        "  if (ld[5] != ''):\n",
        "    bb_map[ld[5]] = ld[2]\n",
        "  if (ld[3] != ''):\n",
        "    bse_map[ld[3]] = ld[2]\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6meFU7Qg8g-"
      },
      "source": [
        "# Function to get text from pdf by OCR\n",
        "\n",
        "def get_ocr_output(pdf_dir,temp_file):\n",
        "  nfile = pdf_dir+temp_file\n",
        "  pdf_id = nfile.strip('.pdf')\n",
        "\n",
        "  filename = nfile\n",
        "  num_pages = get_num_pages(filename)\n",
        "  if num_pages > 150:\n",
        "    process = Popen(['cp',str(filename), large_pdfs],\n",
        "                stdout=subprocess.PIPE, \n",
        "                stderr=subprocess.PIPE)\n",
        "    stdout, stderr = process.communicate()\n",
        "\n",
        "    print(str(pdf_id)+\" copied to large_pdfs folder\")\n",
        "  fp = open(img_path,\"w\")\n",
        "\n",
        "  # Convert a PDF into a list of single page .jpg files\n",
        "  # needed for Tesseract\n",
        "  for num in range(1,num_pages+1):\n",
        "    page = convert_from_path(filename, 500,first_page=num,last_page=num)\n",
        "    page[0].save(temp_dir+temp_file+'_'+str(num)+'.jpg', 'JPEG')\n",
        "    fp.write(temp_dir+temp_file+'_'+str(num)+'.jpg\\n')\n",
        "  fp.close()\n",
        "  list_text_files = os.listdir(temp_dir)\n",
        "  print(\"No of Pages in this pdf: \"+str(len(list_text_files)))\n",
        "\n",
        "  text_file = text_dir+str(temp_file.strip('.pdf'))\n",
        "  \n",
        "  # Apply tesseract OCR\n",
        "  process = Popen(['tesseract', img_path, text_file, '--oem', '3', '-l','eng+hin', '-c', 'page_separator=♞'],\n",
        "                stdout=subprocess.PIPE, \n",
        "                stderr=subprocess.PIPE)\n",
        "  stdout, stderr = process.communicate()\n",
        "  for i in list_text_files:\n",
        "      os.remove(temp_dir+str(i))\n",
        "  os.remove(img_path)\n",
        "  # Need to return the path to the text file containg the text output\n",
        "  return text_file+'.txt'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWdGRrDgGzWN"
      },
      "source": [
        "#### Get Persons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Mc6PBk_G1nh"
      },
      "source": [
        "# Function to get person names from a text file (single page)\n",
        "def get_person_list(filename):\n",
        "  f = open(test_txt,'r')\n",
        "  text = 'While in France, Christine Lagarde along with Sahil Makhija and Bhrugesh Parsawala and RAVI SWAMINATHAN discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
        "  text_test = f.read()\n",
        "  i = 0\n",
        "  persons = []\n",
        "  classified_test = st.tag(word_tokenize(text_test))\n",
        "  # Extracting relevant person names in list format\n",
        "  while i < len(classified_test):\n",
        "    if 'PERSON' in classified_test[i][1]:\n",
        "      j = i\n",
        "      name = \"\"\n",
        "      while 'PERSON' == classified_test[j][1]:\n",
        "        name += classified_test[j][0]+\" \"\n",
        "        j += 1\n",
        "      i = j\n",
        "      name = name[:-1]\n",
        "      persons.append(name)\n",
        "    i += 1\n",
        "  if len(persons) > 0:\n",
        "    print(persons)\n",
        "    return ';'.join(persons)\n",
        "  else:\n",
        "    return \"NA\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udPUMQA_l-7a"
      },
      "source": [
        "#### Get Rating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kc5OXBLumBED"
      },
      "source": [
        "# Page No has to start from 1\n",
        "# convert a page in pdf to jpg\n",
        "def get_pdfpage_jpg(filename,new_file_name,page_no):\n",
        "  page = convert_from_path(filename, 500,first_page=page_no,last_page=page_no)\n",
        "  page[0].save(new_file_name, 'JPEG')\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SN6lymjjnCTd"
      },
      "source": [
        "# Function to get text data from image by OCR\n",
        "def get_ocr_page(filename,new_txt_name,page_no):\n",
        "  new_jpg_name = filename.strip('.pdf')+'.jpg'\n",
        "\n",
        "  get_pdfpage_jpg(filename,new_txt_name+'.jpg',page_no)\n",
        "  process = Popen(['tesseract', new_txt_name+'.jpg', new_txt_name, '--oem', '3', '-l','eng', '-c', 'page_separator=♞'],\n",
        "                stdout=subprocess.PIPE, \n",
        "                stderr=subprocess.PIPE)\n",
        "  stdout, stderr = process.communicate()\n",
        "  # print(stdout)\n",
        "  # print(stderr)\n",
        "  return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWWZ-c4ZtST9"
      },
      "source": [
        "# Function to get the recommended rating from the pdf\n",
        "def getRating(filename):\n",
        "\n",
        "  # Check whether the pdf is unrated or not\n",
        "  arg = ['grep','--max-count', '5', '-o', f\"{'(NOT RATED|Under Review)'}\", filename]\n",
        "  companies_check = subprocess.run(arg, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "  print(companies_check)\n",
        "  if len(companies_check) > 0:\n",
        "    return \"NA\"\n",
        "\n",
        "  # List of rating keywords to be searched in the pdf\n",
        "  keywords = ['BUY','Buy','SELL','Sell','HOLD','ADD','Add','REDUCE','Reduce','Hold','Neutral','NEUTRAL','Outperform','OUTPERFORM','Underperform','UNDERPERFORM','Suspended','SUSPENDED','UNDERPERFORMER','Underperformer','OUTPERFORMER','Outperformer','Accumulate','ACCUMULATE']\n",
        "  keywords_regex = \"\"\n",
        "  for i in range(len(keywords)):\n",
        "    keywords_regex += keywords[i]\n",
        "    if i <= len(keywords) - 1:\n",
        "      keywords_regex += \"|\"\n",
        "  keywords_regex = \"\\\"[^\\w](\"+keywords_regex+\")[^\\w]\\\"\"\n",
        "\n",
        "  f = open(filename,  \"r\")\n",
        "  for l in f.readlines():\n",
        "    for keyword in  keywords:\n",
        "      if re.search(r\"\\W{}\\W\".format(keyword),l):\n",
        "        res = keyword\n",
        "        return res\n",
        "  # In case the rating keyword is not found, leave blank\n",
        "  return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCOkJfDTEHti"
      },
      "source": [
        "#### Target Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VgIEaCTX3uMP"
      },
      "source": [
        "# extract the word which denotes the price to be obtained\n",
        "def get_money_amt(prices):\n",
        "  for j in prices.split(' '):\n",
        "    if any(char.isdigit() for char in j):\n",
        "      return j"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D758Kt3X7zNk"
      },
      "source": [
        "# Out of all the amounts, get one which is maximum\n",
        "# Applied across all target prices\n",
        "def get_max_amt(amts):\n",
        "  maxstr = \"\"\n",
        "  maxa = -1\n",
        "  print(amts)\n",
        "  for amt in amts:\n",
        "    a = \"\"\n",
        "    if amt is not None:\n",
        "      for char in amt:\n",
        "        if char.isdigit():\n",
        "          a = a + char\n",
        "      if int(a) > maxa:\n",
        "        maxstr = amt\n",
        "        maxa = int(a)\n",
        "  return maxstr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mR6NEnxiYwQO"
      },
      "source": [
        "# Save the first page of pdf as a separate pdf\n",
        "def getFirstPage(file_path, new_file_path):\n",
        "  inputpdf = PdfFileReader(open(file_path, \"rb\"))\n",
        "\n",
        "  # if the pdf is encrypted\n",
        "  # work with the original file itself\n",
        "  if inputpdf.isEncrypted:\n",
        "    return False\n",
        "  output = PdfFileWriter()\n",
        "  x = inputpdf.getPage(0)\n",
        "  output.addPage(x)\n",
        "  with open(new_file_path, \"wb\") as outputStream:\n",
        "      output.write(outputStream)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ5RnpMFOcQu"
      },
      "source": [
        "def fetch_target_price(cmd_tool,file_path):\n",
        "  # For PDF data\n",
        "  if cmd_tool == \"pdfgrep\":\n",
        "    is_encrypted = getFirstPage(file_path, large_pdfs+\"check_first_0.pdf\")\n",
        "    # if the pdf is encrypted\n",
        "    # work with the original file itself\n",
        "    if is_encrypted:\n",
        "      nfile_path = file_path\n",
        "    else:\n",
        "      nfile_path = large_pdfs+\"check_first_0.pdf\"\n",
        "    # if \n",
        "  # For Text Data\n",
        "  else:\n",
        "    nfile_path = file_path\n",
        "  \n",
        "  # Check whether the pdf is unrated or not\n",
        "  arg = [cmd_tool,'--max-count', '5', '-o', f\"{'(NOT RATED|Under Review)'}\", nfile_path]\n",
        "  companies_check = subprocess.run(arg, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "  if len(companies_check) > 0:\n",
        "    return \"NA\"\n",
        "\n",
        "  # Price regex used for extracting the target price data\n",
        "  price_regex = '([eE]xit [pP]rice|[tT]arget [pP]rice|[pP]rice [tT]arget|Fair Value|[pP]rice|[tT]arget[s]?|TP|TARGET|[pP]rice [oO]bjective|value the stock)[\\t\\ ]*(\\([^/\\n]*\\))?:?[\\t\\ ]*-?[\\t\\ ]*(of|at|to)?[\\t\\ ]*(RS|Rs|INR|Inr|`|W|\\|)?[.]?[\\t\\ ]*[0-9,.]+[\\t\\ ]*(RS|Rs|INR|Inr|`|W|\\|)?'\n",
        "  arg = [cmd_tool,'--max-count', '5', '-o', f\"{price_regex}\", file_path]\n",
        "  companies = subprocess.run(arg, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "  target_check = ['target','Target','TARGET','value','objective','Objective','value','TP','Fair','fair']\n",
        "  target_achieved = False\n",
        "  amts = []\n",
        "\n",
        "  # If there are phrases containing target price, collect the price data\n",
        "  # and compare with other target prices for the maximum one\n",
        "  for l in range(len(companies)):\n",
        "    # amts = []\n",
        "    for t in target_check:\n",
        "      if t in companies[l] and any(char.isdigit() for char in companies[l]):\n",
        "        target_achieved = True\n",
        "        amt = get_money_amt(companies[l])\n",
        "        amts.append(amt)\n",
        "        break\n",
        "  if target_achieved:\n",
        "    return ''.join(get_max_amt(amts).split(','))\n",
        "  # If not the case, compare on others to get the maximum price data\n",
        "  else:\n",
        "    amts = []\n",
        "    for l in range(len(companies)):\n",
        "      amts.append(get_money_amt(companies[l]))\n",
        "    res = get_max_amt(amts)\n",
        "    return ''.join(res.split(','))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZICsPkBKEKN6"
      },
      "source": [
        "# Wrapper function to handle both text and pdf data\n",
        "def get_target_price(pdf_dir,temp_file,text_pdf_path):\n",
        "  if text_pdf_path != \"\":\n",
        "    # ToDo\n",
        "    return fetch_target_price(\"grep\",text_pdf_path)\n",
        "    # pass\n",
        "  else:\n",
        "    nfile = pdf_dir+temp_file\n",
        "    return fetch_target_price(\"pdfgrep\",nfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41POamTPD5Od"
      },
      "source": [
        "#### Auditing Company"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wkjNGSXseQng"
      },
      "source": [
        "# Function to get auditing company name\n",
        "def get_aud_company(pdf_dir,temp_file,text_pdf_path):\n",
        "  nfile = pdf_dir + temp_file\n",
        "  # Candidate Regex to get auditing company from email ids of employees\n",
        "  filtered = '[A-Za-z]+@[A-Za-z]+.[A-Za-z]+' # the list of regexes is shortened to one string, to keep the example simple.\n",
        "  # Candidate Regex to get auditing company from its \"pvt ltd\" name\n",
        "  filtered_ltd = '[A-Z][A-Za-z.]+[\\t\\ ]*([A-Za-z.]+[\\t\\ ]*)+([lL]td|[lL]imited)'\n",
        "  arg = ['pdfgrep',  '-o',  f\"{filtered}\", nfile]\n",
        "  arg_ltd = ['pdfgrep',  '-o',  f\"{filtered_ltd}\", nfile]\n",
        "  process_match = subprocess.run(arg, stdout=subprocess.PIPE)\n",
        "  process_match_ltd = subprocess.run(arg_ltd, stdout=subprocess.PIPE)\n",
        "  emails = process_match.stdout.decode(\"utf-8\").split('\\n')\n",
        "  emails_ltd = process_match_ltd.stdout.decode(\"utf-8\").split('\\n')\n",
        "\n",
        "  # If candidate 1 matches\n",
        "  if len(emails) > 1 and emails[0] != '':\n",
        "    print(emails)\n",
        "    return emails[0].split('@')[1].split('.')[0]\n",
        "  # If candidate 2 matches\n",
        "  elif len(emails_ltd) > 1 and emails_ltd[0] != '':\n",
        "    while emails_ltd[-1] == '' and len(emails_ltd) > 1:\n",
        "      emails_ltd.pop()\n",
        "    if len(emails_ltd) == 0:\n",
        "      return ''\n",
        "    else:\n",
        "      return emails_ltd[-1]\n",
        "\n",
        "  elif text_pdf_path != \"\":\n",
        "    # the pdf may not be recognizable by pdfgrep\n",
        "    # in this case, we use tesseract ocr to extract company names\n",
        "    # text_path = get_ocr_output(pdf_dir,temp_file)\n",
        "    arg = ['grep',  '-o','-E',  f\"{filtered}\", text_pdf_path]\n",
        "    process_match = subprocess.run(arg, stdout=subprocess.PIPE)\n",
        "    emails_text = process_match.stdout.decode(\"utf-8\").split('\\n')\n",
        "    print(emails_text)\n",
        "    return emails_text[0].split('@')[1].split('.')[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPd6xa5uD9Wp"
      },
      "source": [
        "#### Company being Audited"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1JwjWfx1ZABW"
      },
      "source": [
        "# helper function to get intersection of two lists\n",
        "def intersection(lst1, lst2): \n",
        "    return list(set(lst1) & set(lst2)) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCXCdGpKSRHE"
      },
      "source": [
        "def get_company_name(pdf_dir,temp_file,text_pdf_path):\n",
        "  if text_pdf_path != \"\":\n",
        "    return \"Negative\"\n",
        "    pass\n",
        "  else:\n",
        "    nfile = pdf_dir+temp_file\n",
        "    # Getting company names from BloomBerg Codes\n",
        "    bloomberg_regex = '[A-Z]+[\\t\\ ]*:?(IN|IB)[^A-Za-z0-9]'\n",
        "    # Getting company names from NSE/BSE codes\n",
        "    nse_regex = '(NSE|BSE)[\\t\\ ]*(SYMBOL|[Cc]ode|[tT]icker)?:?[\\t\\ ]*([A-Z]+|[0-9]+)[^a-z]'\n",
        "    # If the above two don't work, then in some documents, the first occuring Alphabetic code in round caps and capital\n",
        "    # would correspond to bloomberg codes\n",
        "    bracket_bb_regex = '\\([A-Z]+\\)'\n",
        "    arg = ['pdfgrep', '-o', f\"{bloomberg_regex}\", nfile]\n",
        "    arg_nse = ['pdfgrep',  '-o',  f\"{nse_regex}\", nfile]\n",
        "    arg_bb = ['pdfgrep',  '-o',  f\"{bracket_bb_regex}\", nfile]\n",
        "    companies = subprocess.run(arg, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "    # Preprocessing on bloomberg codes\n",
        "    for i in range(len(companies)):\n",
        "      if ':' in companies[i]:\n",
        "        companies[i] = ' '.join(companies[i].split())\n",
        "      if companies[i][-2:] == \"IB\":\n",
        "        companies[i] = companies[i].split('IB')[0]+\"IN\"\n",
        "      elif companies[i][-2:] != \"IN\":\n",
        "        companies[i] = companies[i][:-1]\n",
        "    companies_nse = subprocess.run(arg_nse, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "    companies_bb = subprocess.run(arg_bb, stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')[:-1]\n",
        "    # Case 1\n",
        "    if len(companies) > 0 and intersection(companies,list(bb_map.keys())):# companies[0].split(' ')[0] not in ['CHANGE','IMPORTANT']:\n",
        "      final_list = intersection(companies,list(bb_map.keys()))\n",
        "      return ';'.join([bb_map[i] for i in final_list])\n",
        "    # Case 2\n",
        "    elif len(companies_nse) > 1 and not re.match('(NSE|BSE) INB',companies_nse[0]):\n",
        "      if \"NSEX\" in companies_nse[0] and len(companies_nse) >= 2:\n",
        "        companies_nse[0] = companies_nse[1]\n",
        "      forbid = [\"SYMBOL\",\"Code\",\"code\",\":\",\"ticker\",'Ticker']\n",
        "      for w in forbid:\n",
        "        if w in companies_nse[0]:\n",
        "          companies_nse[0] = ' '.join(companies_nse[0].split(w))\n",
        "      \n",
        "      companies_nse[0] = ' '.join(companies_nse[0].split())\n",
        "      # BSE codes contain only digits and NSE codes contain only capital letters\n",
        "      # checking the company based on that.\n",
        "      if not companies_nse[0][-1].isdigit() and not companies_nse[0][-1].isalpha():\n",
        "        companies_nse[0] = companies_nse[0][:-1]\n",
        "      print(companies_nse[0])\n",
        "      code_arg = ' '.join(companies_nse[0][3:].split())\n",
        "      if code_arg.isdigit():\n",
        "        if code_arg in bse_map.keys():\n",
        "          return bse_map[code_arg]\n",
        "        else:\n",
        "          return \"\"\n",
        "      elif code_arg.isalpha():\n",
        "        if code_arg in nse_map.keys():\n",
        "          return nse_map[code_arg]\n",
        "        else:\n",
        "          return \"\"\n",
        "    # Case 3\n",
        "    else:\n",
        "      if  (len(companies_bb) == 0):\n",
        "        return \"\"\n",
        "      else :\n",
        "        code_arg = companies_bb[0][1:-1]+\" IN\"\n",
        "        if code_arg in bb_map.keys(): \n",
        "          return bb_map[code_arg]\n",
        "        else:\n",
        "          return \"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zY4R1VUuEDkK"
      },
      "source": [
        "#### Main Parsing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-8GzZj-Z3cI"
      },
      "source": [
        "list_files = sorted(os.listdir(pdf_dir))\n",
        "flines = []\n",
        "f = open(csv_file,'w')\n",
        "for idx, nfile in enumerate(list_files):\n",
        "  pdf_id = nfile.strip('.pdf')\n",
        "  print(nfile)\n",
        "  # Company report\n",
        "  if \"[CR]\" in pdf_id:\n",
        "    is_sector = \"NO\"\n",
        "    rec_pdf = subprocess.run(['pdfgrep',  '-o',  f\"the\", pdf_dir+nfile], stdout=subprocess.PIPE).stdout.decode(\"utf-8\").split('\\n')\n",
        "    if len(rec_pdf) == 1 and rec_pdf[0] == '':\n",
        "      text_path = get_ocr_output(pdf_dir,nfile)\n",
        "    else:\n",
        "      text_path = \"\"\n",
        "    get_ocr_page(pdf_dir+nfile,temp_dir+pdf_id+\"_0\",1)\n",
        "    num_p = get_num_pages(pdf_dir+nfile)\n",
        "    rating = getRating(temp_dir+pdf_id+\"_0.txt\")\n",
        "    persons = get_person_list(temp_dir+pdf_id+\"_0.txt\")\n",
        "    target_price = get_target_price(pdf_dir,nfile,text_path)\n",
        "    aud_company = get_aud_company(pdf_dir,nfile)\n",
        "    obj_company = get_company_name(pdf_dir,nfile,text_path)\n",
        "    \n",
        "  # Sector report\n",
        "  elif \"[Sector]\" in pdf_id:\n",
        "    is_sector = \"YES\"\n",
        "    get_ocr_page(pdf_dir+nfile,temp_dir+pdf_id+\"_0\",1)\n",
        "    aud_company = get_aud_company(pdf_dir,nfile)\n",
        "    obj_company = \"None\"\n",
        "    target_price = \"NA\"\n",
        "    rating = \"NA\"\n",
        "    persons = get_person_list(temp_dir+pdf_id+\"_0.txt\")\n",
        "  # the remaining files are marked as others\n",
        "  else:\n",
        "    is_sector = \"NO\"\n",
        "    aud_company = \"Others\"\n",
        "    obj_company = \"None\"\n",
        "    target_price = \"NA\"\n",
        "    rating = \"NA\"\n",
        "    persons = \"NA\"\n",
        "  modified_nfile = ' '.join(pdf_id.split(','))+\".pdf\"\n",
        "  fline = modified_nfile+\",\"+aud_company+\",\"+is_sector+\",\"+obj_company+\",\"+target_price+\",\"+rating+\",\"+persons+'\\n'\n",
        "  print(fline)\n",
        "  f.write(fline)\n",
        "  flines.append(fline)\n",
        "print(flines)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}